# -*- coding: utf-8 -*-
"""RubberLeafeClassification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16m0hl3SU2wOyD0lKRghESiYMdzuj6WVZ
"""

from google.colab import drive
drive.mount('/content/drive')

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.applications import VGG16
from tensorflow.keras.layers import GlobalAveragePooling2D
import matplotlib.pyplot as plt
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from tensorflow.keras.models import load_model
import seaborn as sns
import numpy as np
import os
import cv2

# Define the path to the data directory (update this path)
data_directory = '/content/drive/MyDrive/rubberDiseasesClassifeid'

def adjust_brightness_and_saturation(image):
    # Increase brightness
    image = tf.image.adjust_brightness(image, delta=0.2)
    #image = image * [1, 0, 1]
    # image = tf.image.rgb_to_grayscale(image)
    # image = tf.image.grayscale_to_rgb(image)
    # Increase saturation
    image = tf.image.adjust_saturation(image, saturation_factor=1.2)

    return image

datagen = ImageDataGenerator(
    rescale=1.0/255,
    brightness_range=[1.2, 1.2],                # Increase brightness
    preprocessing_function=adjust_brightness_and_saturation, # Custom preprocessing function
    shear_range=0.2,                            # Randomly apply shearing transformations
    zoom_range=0.2,                             # Randomly zoom inside the pictures
    horizontal_flip=True,                       # Randomly flip inputs horizontally
    fill_mode='nearest',                        # Fill in newly created pixels
    validation_split=0.2                        # Split 20% of the data for validation
)

# Create training and validation generators
train_generator = datagen.flow_from_directory(
    data_directory,
    target_size=(250, 250),
    batch_size=32,
    class_mode='categorical',
    subset='training'
)

validation_generator = datagen.flow_from_directory(
    data_directory,
    target_size=(250, 250),
    batch_size=32,
    class_mode='categorical',
    subset='validation'
)

# Display some sample images with labels
sample_images, sample_labels = next(train_generator)
class_indices = train_generator.class_indices
class_labels = {value: key for key, value in class_indices.items()}

plt.figure(figsize=(10, 10))
for i in range(9):
    plt.subplot(3, 3, i + 1)
    plt.imshow(sample_images[i])
    plt.title(class_labels[sample_labels[i].argmax()])
    plt.axis('off')
plt.show()

base_model = VGG16(input_shape=(250,250,3),include_top=False)

for layer in base_model.layers:
  layer.trainable = False

# Create a new model with additional layers
model = Sequential([
    base_model,
    GlobalAveragePooling2D(),
    Dense(32, activation='relu'),
    Dropout(0.5),
    Dense(4, activation='softmax') # 4 classes
])

# Create a sequential model
# model = Sequential([
#     Conv2D(16, (3,3), activation='relu', input_shape=(250, 250, 3)),
#     MaxPooling2D(2, 2),
#     Conv2D(32, (3,3), activation='relu'),
#     MaxPooling2D(2, 2),
#     Conv2D(64, (3,3), activation='relu'),
#     MaxPooling2D(2, 2),
#     Flatten(),
#     Dense(32, activation='relu'),
#     Dropout(0.5),
#     Dense(4, activation='softmax') # 4 classes
# ])

# Early stopping to prevent overfitting
early_stopping = EarlyStopping(monitor='val_loss', patience=6)

# Save the best model during training
model_checkpoint = ModelCheckpoint(
    '/content/drive/MyDrive/ML Models/2_best_leaf_model.h5',
    monitor='val_loss',
    save_best_only=True
)

# Compile the model
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Print the model summary
model.summary()

# Train the model
history = model.fit(
    train_generator,
    epochs=30,
    validation_data=validation_generator,
    callbacks=[early_stopping, model_checkpoint]
)

plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.title('Training and Validation Accuracy')
plt.show()

# Path to the saved best model
saved_model_path = '/content/drive/MyDrive/ML Models/2_best_leaf_model.h5'

# Load the saved model
model = load_model(saved_model_path)

test_images, test_labels = next(validation_generator)

# # Select the first 5 images and labels
# test_images = test_images[:5]
# test_labels = test_labels[:5]

# Select random 5 indexes from the test set
random_indexes = np.random.choice(len(test_images), 5, replace=False)

# Use the random indexes to select the corresponding images and labels
test_images = np.array([test_images[i] for i in random_indexes])
test_labels = np.array([test_labels[i] for i in random_indexes])


# Make predictions
predictions = model.predict(test_images)

# Get the class labels
class_labels = {value: key for key, value in validation_generator.class_indices.items()}

# Display the results
plt.figure(figsize=(12, 12))
for i in range(5):
    plt.subplot(5, 1, i + 1)
    plt.imshow(test_images[i])
    expected_label = class_labels[test_labels[i].argmax()]
    predicted_label = class_labels[predictions[i].argmax()]
    plt.title(f'Expected: {expected_label} | Predicted: {predicted_label}')
    plt.axis('off')
plt.show()

# Define the unique class labels (0 to 3 for four classes)
unique_class_labels = list(range(4))

# Get the true class labels
true_labels = [label.argmax() for label in test_labels]

# Get the predicted class labels
predicted_labels = [prediction.argmax() for prediction in predictions]

# Calculate the confusion matrix, specifying the labels
cm = confusion_matrix(true_labels, predicted_labels, labels=unique_class_labels)

# Get the class names from the class labels
class_names = [class_labels[i] for i in unique_class_labels]

# Create a DataFrame for a better visual representation
import pandas as pd
confusion_df = pd.DataFrame(cm, index=class_names, columns=class_names)

# Display the confusion matrix using Seaborn
plt.figure(figsize=(8, 6))
sns.heatmap(confusion_df, annot=True, cmap='Blues', fmt='g')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

confusion_df = pd.DataFrame(cm, index=class_names, columns=class_names)
print(confusion_df)